import { defineStore } from 'pinia'
// import { i18n } from 'boot/i18n'
import { ref } from 'vue'
import axios from 'axios'
// import notify from 'src/utils/helpers/notify'

export const useArticlesStore = defineStore('articles', {
  state: () => ({
    list: [], // список статей
    selected: {}, // выбранная статья
  }),

  actions: {
    /**
     * Получает список статей.
     * @returns {Promise}
     */
    async index() {
      return axios
        .get('/articles')
        .then(() => {})
        .catch(() => {
          //   notify.error(i18n.global.t('message.error.articles.index'))
          this.list = ref([
            {
              author: {
                id: 1,
                username: 'MaxGoover',
              },
              content:
                'Пару слов про функции потерь.\nФункции потерь можно разделить на выпуклые и невыпуклые. Первый вариант чаще всего встречается в классическом машинном обучении из-за относительной простоты моделей, а также из-за того, что у выпуклых функций локальный минимум по определению является и глобальным (но не обязательно единственным). Невыпуклые функции, в свою очередь, чаще всего встречаются в нейронных сетях из-за их высокой сложности и в данном случае поиск глобального минимума является трудной задачей, поэтому на практике здесь также используются методы из случая с выпуклыми функциями из-за их способности хорошо аппроксимировать стационарные точки в невыпуклом случае. Проще говоря, методы для выпуклого случая могут хорошо сходиться в локальном оптимуме для невыпуклого случая за приемлемое время, значение которого близко к глобальному.\nКлассический градиентный спуск\nНачнём с того, что если градиент — это вектор наибыстрейшего возрастания функции, то антиградиент — вектор наибыстрейшего убывания, и именно при движении в данную сторону будет расположена минимальная ошибка модели. Тогда градиентный спуск можно определить как численный метод итеративной оптимизации для нахождения весов (коэффициентов) модели путём минимизации её ошибки, представленной в виде функции потерь. Пример работы данного алгоритма на линейной и логистической регрессий с нуля можно посмотреть здесь и здесь.',
              countComments: 3,
              created_at: '2024-04-15 16:46:01',
              id: 1,
              title: 'Методы оптимизации в машинном и глубоком обучении. От простого к сложному',
              updated_at: '2024-04-15 16:46:01',
            },
            {
              author: {
                id: 2,
                username: 'John Doe',
              },
              content:
                'Исследователи из Массачусетского технологического института (MIT) делают прорыв в области робототехники, создавая роботов, способных как слизь, изменять свою форму для выполнения разнообразных задач – от банального преодоления препятствий до серьезных медицинских манипуляций (в перспективе!). Обучение роботов, состоящих из множества мелких ”мышц”, традиционными методами невозможно, поэтому исследователи применили метод обучения с подкреплением, начиная с управления целыми “группами мышц”, постепенно уточняя контроль до уровня отдельного “кусочка” мышцы.',
              countComments: 4,
              created_at: '2024-04-14 16:46:01',
              id: 2,
              title:
                'Будущее робототехники: революционный подход к обучению мягких роботов с помощью ML',
              updated_at: '2024-04-14 16:46:01',
            },
            {
              author: {
                id: 1,
                username: 'MaxGoover',
              },
              content:
                '«Треки» - одна из интересных головоломок ежегодного квеста Puzzle Hunt Мельбурнского Университета.\n Эта головоломка представляет собой два поля одинакового размера, разделенные на клетки горизонтальными и вертикальными линиями.\nТаким образом, каждое поле состоит из 6 рядов и 16 столбцов. Помимо этого, оба поля вертикально разделены посередине на две равные части. В верхнем поле есть клетка, обозначенная как start, и клетка, обозначенная как finish. Каждому столбцу верхнего поля, а также каждой строке в левой и правой его части сопоставлена цифра. Нижнее поле содержит большое число цифр от 1 до 9, которые довольно хаотично расставлены в некоторых его клетках.',
              countComments: 5,
              created_at: '2024-04-13 16:46:01',
              id: 3,
              title: 'Решение головоломки из университетского квеста с помощью Python',
              updated_at: '2024-04-13 16:46:01',
            },
          ])
        })
    },
    async show(id) {
      return axios
        .get(`/articles/${id}`)
        .then(() => {})
        .catch(() => {
          //   notify.error(i18n.global.t('message.error.articles.index'))
          this.setSelected({
            author: {
              id: 1,
              username: 'MaxGoover',
            },
            content:
              'Пару слов про функции потерь.\nФункции потерь можно разделить на выпуклые и невыпуклые. Первый вариант чаще всего встречается в классическом машинном обучении из-за относительной простоты моделей, а также из-за того, что у выпуклых функций локальный минимум по определению является и глобальным (но не обязательно единственным). Невыпуклые функции, в свою очередь, чаще всего встречаются в нейронных сетях из-за их высокой сложности и в данном случае поиск глобального минимума является трудной задачей, поэтому на практике здесь также используются методы из случая с выпуклыми функциями из-за их способности хорошо аппроксимировать стационарные точки в невыпуклом случае. Проще говоря, методы для выпуклого случая могут хорошо сходиться в локальном оптимуме для невыпуклого случая за приемлемое время, значение которого близко к глобальному.\nКлассический градиентный спуск\nНачнём с того, что если градиент — это вектор наибыстрейшего возрастания функции, то антиградиент — вектор наибыстрейшего убывания, и именно при движении в данную сторону будет расположена минимальная ошибка модели. Тогда градиентный спуск можно определить как численный метод итеративной оптимизации для нахождения весов (коэффициентов) модели путём минимизации её ошибки, представленной в виде функции потерь. Пример работы данного алгоритма на линейной и логистической регрессий с нуля можно посмотреть здесь и здесь.',
            countComments: 3,
            created_at: '2024-04-15 16:46:01',
            id: 1,
            title: 'Методы оптимизации в машинном и глубоком обучении. От простого к сложному',
            updated_at: '2024-04-15 16:46:01',
          })
        })
    },
    /**
     * Изменяет список статей.
     * @param {Array} - массив объектов статей
     * @returns {void}
     */
    setList(articles) {
      this.list = articles
    },
    /**
     * Выбирает статью.
     * @param {Object} - объект статьи
     * @returns {void}
     */
    setSelected(article) {
      this.selected = article
    },
  },
})
